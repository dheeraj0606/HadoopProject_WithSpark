{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also see: https://datascience.ibm.com/docs/content/analyze-data/python_load.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: This notebook downloads files from WebHDFS to the spark local file system.  It does not clean up these files.  You will need to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credentials - keep this secret!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Cluster number, e.g. 100000\n",
    "cluster  = ''\n",
    "\n",
    "# Cluster username\n",
    "username = ''\n",
    "\n",
    "# Cluster password\n",
    "password = ''\n",
    "\n",
    "# file path in HDFS\n",
    "webhdfs_filepath = 'yourpath/yourfile.txt'\n",
    "\n",
    "# where to save the file in the spark service file system\n",
    "local_filepath = 'yourfile.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your custom code to read_csv_lines for processing your datafile    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "host = 'ehaasp-{0}-mastermanager.bi.services.bluemix.net'.format(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Code to connect to BigInsights on Cloud via WebHDFS - don't change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "import datetime\n",
    "    \n",
    "print('READ FILE START: {0}'.format(datetime.datetime.now()))\n",
    "\n",
    "chunk_size = 200000000 # Read in 200 Mb chunks\n",
    "\n",
    "url = \"https://{0}:8443/gateway/default/webhdfs/v1/{1}?op=OPEN\".format(host, webhdfs_filepath)\n",
    "\n",
    "# note SSL verification is been disabled\n",
    "r = requests.get(url, \n",
    "                 auth=(username, password), \n",
    "                 verify=False, \n",
    "                 allow_redirects=True, \n",
    "                 stream=True)\n",
    "chunk_num = 1\n",
    "with open(local_filepath, 'wb') as f:\n",
    "    for chunk in r.iter_content(chunk_size):\n",
    "        if chunk: # filter out keep-alive new chunks\n",
    "           print('{0} writing chunk {1}'.format(datetime.datetime.now(), chunk_num))\n",
    "           f.write(chunk)\n",
    "           chunk_num = chunk_num + 1\n",
    "        \n",
    "print('READ FILE END: {0}'.format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df = sqlContext.read.format('com.databricks.spark.csv') \\\n",
    "            .options(header='false', inferschema='true', delimiter='|') \\\n",
    "            .load(local_filepath)\n",
    "df.cache()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
